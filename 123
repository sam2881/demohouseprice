from pyspark.sql.functions import col, explode_outer

def explode_and_track(df, paths):
    exploded_info = []
    for path in paths:
        exploded_column = f"{path.split('.')[-1]}_exploded"
        df = df.withColumn(exploded_column, explode_outer(col(path)))
        exploded_info.append((path, exploded_column))
    return df, exploded_info

def expand_structs(df, paths):
    select_expressions = []
    for path, alias in paths:
        subfields = df.schema[path].dataType.elementType if isinstance(df.schema[path].dataType, ArrayType) else df.schema[path].dataType
        if isinstance(subfields, StructType):
            for field in subfields.fields:
                full_path = f"{alias}.{field.name}"
                new_alias = full_path.replace('.', '_')
                df = df.withColumn(new_alias, col(full_path))
                select_expressions.append(f"col('{new_alias}').alias('{new_alias}')")
    return df, select_expressions

def generate_select_statement(select_expressions):
    return f"df.select(\n    {',\n    '.join(select_expressions)}\n).show(truncate=False)"

# Usage:
df, exploded_info = explode_and_track(df, ["path_to_outer_array"])
df, select_expressions = expand_structs(df, exploded_info)
select_statement = generate_select_statement(select_expressions)
print(select_statement)

# Execute the select statement if needed:
exec(select_statement)
