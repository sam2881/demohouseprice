import ast
from datetime import datetime, timedelta
from pyspark.sql import SparkSession
from pytz import timezone

# Define local timezone
local_tz = timezone('Asia/Kolkata')  # Example timezone, adjust as needed

# Initialize Spark session
spark = SparkSession.builder \
    .appName("Fetch Run IDs") \
    .getOrCreate()

def get_start_end_dates_and_run_ids(**kwargs):
    # Retrieve and parse the DAG run configuration
    dag_run_conf = kwargs["dag_run_conf"]
    ti = kwargs["ti"]

    try:
        conf = ast.literal_eval(dag_run_conf)
        start_date = conf.get("start_date", (datetime.now(local_tz) - timedelta(1)).strftime("%Y-%m-%d"))
        end_date = conf.get("end_date", (datetime.now(local_tz) - timedelta(1)).strftime("%Y-%m-%d"))
    except KeyError:
        start_date = (datetime.now(local_tz) - timedelta(1)).strftime("%Y-%m-%d")
        end_date = (datetime.now(local_tz) - timedelta(1)).strftime("%Y-%m-%d")

    # Query Iceberg table to fetch run_ids based on the date range
    df = spark.read.format("iceberg").load("iceberg_table_name")
    run_ids_df = df.filter((df.reportindate >= start_date) & (df.reportindate <= end_date))
    run_ids = [row.run_id for row in run_ids_df.select("run_id").distinct().collect()]

    # Push the start_date, end_date, and list of run_ids to XCom
    ti.xcom_push(key='start_date', value=start_date)
    ti.xcom_push(key='end_date', value=end_date)
    ti.xcom_push(key='run_ids', value=run_ids)

    return {
        "start_date": start_date,
        "end_date": end_date,
        "run_ids": run_ids
    }
