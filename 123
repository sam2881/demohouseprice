from pyspark.sql.functions import col, explode_outer

def apply_explode(df, paths):
    exploded_dfs = []
    for path in paths:
        # Split the path and find the field in the schema
        parts = path.split('.')
        field = df.schema
        try:
            for part in parts:
                # find field in StructType
                field = field[part] if isinstance(field, StructType) else field.dataType[part]
        except TypeError:
            # Skip if the path does not lead to an ArrayType
            continue
        
        # Check if the field's dataType is an ArrayType and explode it
        if isinstance(field.dataType, ArrayType):
            exploded_col_name = camel_to_snake(parts[-1] + "_exploded")
            df = df.withColumn(exploded_col_name, explode_outer(col(path)))
            exploded_dfs.append((exploded_col_name, path))
    return df, exploded_dfs

# The rest of your code remains the same
