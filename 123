from pyspark.sql.functions import col, explode_outer

def generate_explode_and_select(df, path_prefix='', exploded_info=[]):
    new_columns = []
    for field in df.schema.fields:
        current_path = f"{path_prefix}.{field.name}" if path_prefix else field.name
        
        if isinstance(field.dataType, ArrayType):
            exploded_name = f"{field.name}_exploded"
            if path_prefix:
                exploded_name = f"{path_prefix.replace('.', '_')}_{exploded_name}"
                
            df = df.withColumn(exploded_name, explode_outer(col(current_path)))
            exploded_info.append((current_path, exploded_name))
            new_columns += generate_explode_and_select(df, exploded_name, exploded_info)
        
        elif isinstance(field.dataType, StructType):
            new_columns += generate_explode_and_select(df, current_path, exploded_info)
        else:
            if path_prefix:
                select_expr = col(current_path).alias(path_prefix.replace('.', '_') + '_' + field.name)
            else:
                select_expr = col(current_path).alias(field.name)
            new_columns.append(select_expr)
    
    return new_columns

# Use the function
exploded_info = []
select_columns = generate_explode_and_select(df, exploded_info=exploded_info)
df_final = df.select(*select_columns)
df_final.show(truncate=False)
print([c for c in select_columns])
