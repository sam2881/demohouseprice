from pyspark.sql.functions import col, explode_outer
from pyspark.sql import SparkSession
import re

def camel_to_snake(name):
    s1 = re.sub('(.)([A-Z][a-z]+)', r'\1_\2', name)
    return re.sub('([a-z0-9])([A-Z])', r'\1_\2', s1).lower()

def get_field(schema, part):
    for field in schema.fields:
        if field.name == part:
            return field
    return None

def apply_nested_explode_and_select(df, paths):
    exploded_info = []
    select_expressions = []

    for path in paths:
        sub_parts = path.split('.')
        current_path = ""
        current_schema = df.schema
        
        for i, part in enumerate(sub_parts):
            field = get_field(current_schema, part)
            if field is None:
                raise ValueError(f"No field found for part '{part}' in path '{path}'")
            
            if isinstance(field.dataType, StructType):
                current_schema = field.dataType
                current_path = current_path + "." + part if current_path else part
            elif isinstance(field.dataType, ArrayType) and isinstance(field.dataType.elementType, StructType):
                if current_path:
                    complete_path = current_path + "." + part
                else:
                    complete_path = part
                exploded_column_name = camel_to_snake(part + '_exploded')
                if complete_path not in exploded_info:
                    df = df.withColumn(exploded_column_name, explode_outer(col(complete_path)))
                    exploded_info.append(complete_path)
                current_schema = field.dataType.elementType
                current_path = exploded_column_name

        # Generating select expressions for all fields in the final exploded column
        if isinstance(field.dataType, ArrayType) and isinstance(field.dataType.elementType, StructType):
            for struct_field in field.dataType.elementType.fields:
                select_expressions.append(
                    f"col('{current_path}.{struct_field.name}').alias('{camel_to_snake(struct_field.name)}')"
                )

    # Generate the select statement
    select_expr_string = ",\n    ".join(select_expressions)
    select_statement = f"df.select(\n    {select_expr_string}\n).show(truncate=False)"
    return select_statement

# Example usage:
spark = SparkSession.builder.appName("Nested Explode and Select Example").getOrCreate()

# Assuming df is your DataFrame and you have paths like below
paths = [
    "individualCoreIdentity.individualDemographic",
    "individualCoreIdentity.productEntitlement.individualPreference",
    "individualCoreIdentity.productEntitlement.individualPreference.preferenceValue"
]

# Load your data into df, then apply the function
# df = spark.read...
select_statement = apply_nested_explode_and_select(df, paths)

print(select_statement)
spark.stop()
