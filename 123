from pyspark.sql.functions import col, explode_outer
from pyspark.sql.types import StructType, ArrayType

def flatten_json(df):
    """
    Flattens a DataFrame with complex nested fields (Arrays and Structs) by converting them into individual columns
    and builds a customizable select statement.
    
    Parameters:
    - df: The input DataFrame with complex nested fields.
    
    Returns:
    - The flattened DataFrame with all complex fields expanded into separate columns.
    - Prints the select statement for further customization.
    """
    select_expressions = []  # List to hold select expressions for final DataFrame viewing
    complex_fields = dict([(field.name, field.dataType) for field in df.schema.fields
                           if isinstance(field.dataType, (ArrayType, StructType))])

    while complex_fields:
        col_name = list(complex_fields.keys())[0]
        print("Processing: " + col_name + " Type: " + str(type(complex_fields[col_name])))

        if isinstance(complex_fields[col_name], StructType):
            expanded = [col(col_name + '.' + k).alias(col_name + '_' + k) for k in [n.name for n in complex_fields[col_name]]]
            df = df.select("*", *expanded).drop(col_name)
            select_expressions.extend([f"{col_name}_{n.name}" for n in complex_fields[col_name]])

        elif isinstance(complex_fields[col_name], ArrayType):
            exploded_col_name = col_name + "_exploded"
            df = df.withColumn(exploded_col_name, explode_outer(col(col_name)))
            if isinstance(complex_fields[col_name].elementType, StructType):
                for subfield in complex_fields[col_name].elementType.fields:
                    new_col_name = exploded_col_name + '_' + subfield.name
                    df = df.withColumn(new_col_name, col(exploded_col_name + '.' + subfield.name))
                    select_expressions.append(new_col_name)
            else:
                select_expressions.append(exploded_col_name)

        complex_fields = dict([(field.name, field.dataType) for field in df.schema.fields
                               if isinstance(field.dataType, (ArrayType, StructType))])

    # Build and print the select statement for manual editing if necessary
    select_clause = ",\n    ".join(f"col('{expr}').alias('{expr}')" for expr in select_expressions)
    select_statement = f"df.select(\n    {select_clause}\n).show(truncate=False)"
    print(select_statement)

    return df, select_statement  # Return the DataFrame and the select statement

# Example of usage, assuming 'spark' is a SparkSession and 'df' is your DataFrame:
df, select_statement = flatten_json(df)
exec(select_statement)  # Execute the select statement to view the DataFrame
