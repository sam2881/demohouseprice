import json
import re
import yaml
from pathlib import Path
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, explode_outer
from pyspark.sql.types import *
import logging
from copy import deepcopy

logging.basicConfig(level=logging.INFO)

def debug_df(df, step_desc):
    """
    Print schema and data of the DataFrame for debugging purposes.

    Args:
        df (DataFrame): The DataFrame to debug.
        step_desc (str): Description of the current step in processing.
    """
    try:
        print(f"\n--- {step_desc} ---")
        df.printSchema()
        df.show(truncate=False)
    except Exception as e:
        logging.error(f"Error during debugging DataFrame at step '{step_desc}': {e}")
    return df

def read_json_file(file_path):
    """
    Read and return the JSON schema from a file.

    Args:
        file_path (str): Path to the JSON file.

    Returns:
        dict: JSON schema as a dictionary.
    """
    try:
        with open(file_path, 'r') as file:
            return json.load(file)
    except Exception as e:
        logging.error(f"Error reading JSON file at '{file_path}': {e}")
        raise

def camel_to_snake(name):
    """
    Convert camelCase string to snake_case string.

    Args:
        name (str): The camelCase string.

    Returns:
        str: The snake_case string.
    """
    try:
        s1 = re.sub('(.)([A-Z][a-z]+)', r'\1_\2', name)
        return re.sub('([a-z0-9])([A-Z])', r'\1_\2', s1).lower()
    except Exception as e:
        logging.error(f"Error converting '{name}' from camelCase to snake_case: {e}")
        raise

def generate_pyspark_schema(json_schema):
    """
    Generate a PySpark schema from a JSON schema dictionary.

    Args:
        json_schema (dict): JSON schema dictionary.

    Returns:
        StructType: PySpark StructType schema.
    """
    try:
        definitions = json_schema.get('definitions', {})
        main_schema = json_schema if 'type' in json_schema else json_schema.get('properties', {})
        
        def convert_to_spark_schema(schema):
            if isinstance(schema, dict):
                data_type = schema.get('type', 'string').lower()
                if data_type == 'object':
                    fields = [StructField(field, convert_to_spark_schema(prop), True)
                              for field, prop in schema.get('properties', {}).items() if convert_to_spark_schema(prop)]
                    return StructType(fields)
                elif data_type == 'array':
                    element_schema = schema.get('items', {})
                    spark_element_schema = convert_to_spark_schema(element_schema)
                    return ArrayType(spark_element_schema, True) if spark_element_schema else None
                elif data_type == 'string':
                    return StringType()
                elif data_type == 'integer':
                    return IntegerType()
                elif data_type == 'boolean':
                    return BooleanType()
                elif data_type == 'number':
                    return DoubleType()
                else:
                    return StringType()
            elif isinstance(schema, list):
                return convert_to_spark_schema(schema[0]) if schema else None
            return None
        
        return convert_to_spark_schema(main_schema)
    except Exception as e:
        logging.error(f"Error generating PySpark schema: {e}")
        raise

def generate_schema_file(schema, schema_file_path):
    """
    Generate a Python file that contains the schema definition.

    Args:
        schema (StructType): PySpark StructType schema.
        schema_file_path (str): Path to the output Python file.
    """
    try:
        logging.info("Generating schema code.")
        schema_code = f"from pyspark.sql.types import StructType\n\n"
        schema_json = json.dumps(schema.jsonValue(), indent=4).replace('true', 'True').replace('false', 'False')
        schema_code += f"def get_schema():\n"
        schema_code += f"    return StructType.fromJson({schema_json})\n"
        
        logging.info(f"Writing schema code to file: {schema_file_path}")
        with open(schema_file_path, 'w') as file:
            file.write(schema_code)
        logging.info("Schema code written successfully.")
    except Exception as e:
        logging.error(f"Error generating schema file at '{schema_file_path}': {e}")
        raise

def extract_column_paths(schema, parent_path=""):
    """
    Extract column paths and explode paths from the schema.

    Args:
        schema (StructType): PySpark StructType schema.
        parent_path (str): Path prefix for nested fields.

    Returns:
        tuple: A tuple containing:
            - List[str]: Column paths.
            - List[tuple]: Explode paths (original_path, exploded_path).
    """
    try:
        column_paths = []
        explode_paths = []
        for field in schema.fields:
            current_path = f"{parent_path}.{field.name}" if parent_path else field.name
            if isinstance(field.dataType, StructType):
                nested_paths, nested_explode = extract_column_paths(field.dataType, current_path)
                column_paths.extend(nested_paths)
                explode_paths.extend(nested_explode)
            elif isinstance(field.dataType, ArrayType):
                explode_col_name = f"{camel_to_snake(field.name)}_exploded"
                explode_paths.append((current_path, explode_col_name))
                if isinstance(field.dataType.elementType, StructType):
                    nested_paths, nested_explode = extract_column_paths(field.dataType.elementType, f"{explode_col_name}")
                    column_paths.extend(nested_paths)
                    explode_paths.extend(nested_explode)
            else:
                column_paths.append(current_path)
        return column_paths, explode_paths
    except Exception as e:
        logging.error(f"Error extracting column paths: {e}")
        raise

def generate_select_expression_from_list(column_names, explode_paths):
    """
    Generate a DataFrame select expression with properly aliased columns from a list of column names.

    Args:
        column_names (List[str]): List of column paths.
        explode_paths (List[tuple]): List of explode paths (original_path, exploded_path).

    Returns:
        str: Comma-separated select expressions.
    """
    try:
        seen_aliases = {}
        select_expr = []
        exploded_col_names = {exp[1] for exp in explode_paths}

        for col_name in column_names:
            if isinstance(col_name, str):
                parts = col_name.split('.')
                simple_name = parts[-1]  # Get the last part of the name
                snake_case_name = camel_to_snake(simple_name)  # Convert it to snake_case

                # Avoid aliasing exploded columns with their parent name
                if col_name not in exploded_col_names and snake_case_name in seen_aliases:
                    parent_name = camel_to_snake(parts[-2]) if len(parts) > 1 else None
                    if parent_name:
                        snake_case_name = f"{parent_name}_{snake_case_name}"
                seen_aliases[snake_case_name] = seen_aliases.get(snake_case_name, 0) + 1

                select_expr.append(f"col('{col_name}').alias('{snake_case_name}')")
            else:
                raise ValueError("Column name is not a string.")

        return ",\n".join(select_expr)
    except Exception as e:
        logging.error(f"Error generating select expression: {e}")
        raise

def generate_pyspark_script(config, schema):
    """
    Generate a PySpark script based on the given configuration and schema.

    Args:
        config (dict): Configuration dictionary.
        schema (StructType): PySpark StructType schema.
    """
    try:
        schema_file_path = config['pyspark_schema_path']
        output_file_path = config['pyspark_script_output_path']
        schema_module = Path(schema_file_path).stem

        logging.info(f"Generating schema file at: {schema_file_path}")
        generate_schema_file(schema, schema_file_path)
        logging.info("Schema file generated successfully.")

        logging.info("Extracting column paths and explode paths from the schema.")
        column_paths, explode_paths = extract_column_paths(schema)
        logging.info(f"Column paths: {column_paths}")
        logging.info(f"Explode paths: {explode_paths}")

        # Generate explosion code for arrays
        logging.info("Generating explosion code for arrays.")

        explosion_code = []
        updated_column_paths = deepcopy(column_paths)
        exploded_col_map = {}

        for orig_col, exp_col_name in explode_paths:
            # Add explosion code
            explosion_code.append(f"df = df.withColumn('{exp_col_name}', explode_outer(col('{orig_col}')))")
            explosion_code.append(f"df = debug_df(df, 'After exploding {exp_col_name}')")  # Debug info if needed

            # Update column paths and exploded column map
            exploded_col_map[orig_col] = exp_col_name
            for i, path in enumerate(updated_column_paths):
                if orig_col in path:
                    updated_column_paths[i] = path.replace(orig_col, f"exploded_{exp_col_name}")

        print(f"This is updated_column_paths : {updated_column_paths}")

        # Generate the explosion code string for insertion into the script
        explosion_code_str = "\n".join(explosion_code)
        logging.info(f"Explosion code: {explosion_code_str}")

        # Generate select statement
        select_expr_str = generate_select_expression_from_list(updated_column_paths, explode_paths)

        pyspark_code = f"""
import json
import re
import yaml
from pathlib import Path
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, explode_outer
from pyspark.sql.types import *
import logging
from {schema_module} import get_schema

logging.basicConfig(level=logging.INFO)

def debug_df(df, step_desc):
    print(f"\\n--- {{step_desc}} ---")
    df.printSchema()
    df.show(truncate=False)
    return df

spark = SparkSession.builder.appName("JsonToDataFrame").getOrCreate()

schema = get_schema()
df = spark.read.schema(schema).option("multiline", "true").json("{config['json_data_input_path']}")

{explosion_code_str}

# Select and alias columns as required
select_statement = df.select(
    {select_expr_str}
)

# Show the results
select_statement.show(truncate=False)
"""

        logging.info(f"Generating PySpark script file at: {output_file_path}")
        with open(output_file_path, 'w') as file:
            file.write(pyspark_code)
        logging.info("PySpark script file generated successfully.")
    except Exception as e:
        logging.error(f"Error generating PySpark script: {e}")
        raise

if __name__ == "__main__":
    try:
        config_path = 'config.yaml'
        
        logging.info(f"Reading configuration from: {config_path}")
        with open(config_path, 'r') as file:
            config = yaml.safe_load(file)
        logging.info(f"Configuration read successfully: {config}")

        logging.info(f"Reading JSON schema from: {config['json_schema_path']}")
        json_schema = read_json_file(config['json_schema_path'])
        logging.info("JSON schema read successfully.")

        logging.info("Generating PySpark schema.")
        pyspark_schema = generate_pyspark_schema(json_schema)
        logging.info("PySpark schema generated successfully.")

        logging.info("Generating PySpark script.")
        generate_pyspark_script(config, pyspark_schema)
        logging.info("PySpark script generation completed.")
    except Exception as e:
        logging.error(f"Error in main execution: {e}")
        raise
