from pyspark.sql.functions import col

def generate_simple_select(df):
    select_expressions = []
    try:
        for field in df.schema.fields:
            if not isinstance(field.dataType, (ArrayType, StructType)):
                path = field.name
                # Handle the alias creation to ensure it doesn't include invalid characters or conflicts
                alias = path.replace('.', '_')
                select_expressions.append(f"col('{path}').alias('{alias}')")
    except Exception as e:
        print(f"Error processing schema fields: {e}")

    # Generate the select clause for use in a select statement
    select_clause = ",\n    ".join(select_expressions)
    return select_clause

# Example Usage
# Assuming 'df' is your DataFrame
select_clause = generate_simple_select(df)
print(f"df.select(\n    {select_clause}\n).show(truncate=False)")
