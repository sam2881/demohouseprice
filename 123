from pyspark.sql import DataFrame
from pyspark.sql.functions import col, explode_outer
from pyspark.sql.types import StructType, ArrayType, StructField

def flatten_df(df, prefix=''):
    """
    Recursively flattens a DataFrame and handles arrays by exploding them.

    :param df: Input DataFrame
    :param prefix: Cumulative path to prepend to column names for maintaining hierarchy
    :return: DataFrame with flattened schema
    """
    # Base case: no more StructType or ArrayType to flatten
    complex_fields = [f for f in df.schema.fields if isinstance(f.dataType, (StructType, ArrayType))]
    if not complex_fields:
        return df

    new_columns = []
    for field in df.schema.fields:
        new_col_name = prefix + field.name if prefix else field.name

        if isinstance(field.dataType, StructType):
            # Recursively flatten structs
            flattened_df = flatten_df(df.select(f"{new_col_name}.*"), new_col_name + '_')
            new_columns.extend([col(f.name).alias(new_col_name + '_' + f.name) for f in flattened_df.schema.fields])
        elif isinstance(field.dataType, ArrayType):
            # Explode arrays and flatten the resulting DataFrame recursively
            if isinstance(field.dataType.elementType, StructType):
                df = df.withColumn(new_col_name, explode_outer(col(new_col_name)))
                flattened_df = flatten_df(df.select(f"{new_col_name}.*"), new_col_name + '_')
                new_columns.extend([col(f.name).alias(new_col_name + '_' + f.name) for f in flattened_df.schema.fields])
            else:
                new_columns.append(explode_outer(col(new_col_name)).alias(new_col_name))
        else:
            new_columns.append(col(new_col_name).alias(new_col_name))

    return df.select(new_columns)

# Usage
flattened_df = flatten_df(df)
flattened_df.printSchema()
flattened_df.show(truncate=False)
