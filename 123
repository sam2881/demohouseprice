from pyspark.sql.functions import col, explode_outer

def flatten_df(df):
    """
    Flattens a DataFrame with complex nested fields (Arrays and Structs) by creating individual columns
    for each nested field. The function modifies the DataFrame schema directly to make it flat.

    Parameters:
    - df: The input DataFrame with complex nested fields.
    
    Returns:
    - The flattened DataFrame.
    """
    # Loop over fields in the original DataFrame schema
    for field in df.schema.fields:
        # Flatten StructType
        if isinstance(field.dataType, StructType):
            for subfield in field.dataType.fields:
                # Full path to the subfield
                path = f"{field.name}.{subfield.name}"
                # Create a new column for each subfield
                df = df.withColumn(path.replace('.', '_'), col(path))
            # Drop the original struct column after extracting all subfields
            df = df.drop(field.name)

        # Flatten ArrayType containing StructType
        elif isinstance(field.dataType, ArrayType) and isinstance(field.dataType.elementType, StructType):
            # Explode the arrays first to deal with struct fields inside them
            df = df.withColumn(field.name, explode_outer(col(field.name)))
            for subfield in field.dataType.elementType.fields:
                path = f"{field.name}.{subfield.name}"
                df = df.withColumn(path.replace('.', '_'), col(path))
            # Optionally, drop the original array column after expanding or keep it if needed

    return df

# Example usage
# Assuming 'spark' is a SparkSession object and 'df' is a DataFrame loaded with complex nested JSON data.
df_flat = flatten_df(df)
df_flat.printSchema()
