import json
import re
import yaml
from pathlib import Path
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, explode_outer, concat_ws
from pyspark.sql.types import *
import logging
from copy import deepcopy

logging.basicConfig(level=logging.INFO)

def debug_df(df, step_desc):
    """
    Print schema and data of the DataFrame for debugging purposes.

    Args:
        df (DataFrame): The DataFrame to debug.
        step_desc (str): Description of the current step in processing.
    """
    try:
        print(f"\n--- {step_desc} ---")
        df.printSchema()
        df.show(truncate=False)
    except Exception as e:
        logging.error(f"Error during debugging DataFrame at step '{step_desc}': {e}")
    return df

def read_json_file(file_path):
    """
    Read and return the JSON schema from a file.

    Args:
        file_path (str): Path to the JSON file.

    Returns:
        dict: JSON schema as a dictionary.
    """
    try:
        with open(file_path, 'r') as file:
            return json.load(file)
    except Exception as e:
        logging.error(f"Error reading JSON file at '{file_path}': {e}")
        raise

def camel_to_snake(name):
    """
    Convert camelCase string to snake_case string.

    Args:
        name (str): The camelCase string.

    Returns:
        str: The snake_case string.
    """
    try:
        s1 = re.sub('(.)([A-Z][a-z]+)', r'\1_\2', name)
        return re.sub('([a-z0-9])([A-Z])', r'\1_\2', s1).lower()
    except Exception as e:
        logging.error(f"Error converting '{name}' from camelCase to snake_case: {e}")
        raise

def generate_pyspark_schema(json_schema):
    """
    Generate a PySpark schema from a JSON schema dictionary.

    Args:
        json_schema (dict): JSON schema dictionary.

    Returns:
        StructType: PySpark StructType schema.
    """
    try:
        definitions = json_schema.get('definitions', {})
        main_schema = json_schema if 'type' in json_schema else json_schema.get('properties', {})
        
        def convert_to_spark_schema(schema):
            if isinstance(schema, dict):
                data_type = schema.get('type', 'string').lower()
                if data_type == 'object':
                    fields = [StructField(field, convert_to_spark_schema(prop), True)
                              for field, prop in schema.get('properties', {}).items() if convert_to_spark_schema(prop)]
                    return StructType(fields)
                elif data_type == 'array':
                    element_schema = schema.get('items', {})
                    spark_element_schema = convert_to_spark_schema(element_schema)
                    return ArrayType(spark_element_schema, True) if spark_element_schema else None
                elif data_type == 'string':
                    return StringType()
                elif data_type == 'integer':
                    return IntegerType()
                elif data_type == 'boolean':
                    return BooleanType()
                elif data_type == 'number':
                    return DoubleType()
                else:
                    return StringType()
            elif isinstance(schema, list):
                return convert_to_spark_schema(schema[0]) if schema else None
            return None
        
        return convert_to_spark_schema(main_schema)
    except Exception as e:
        logging.error(f"Error generating PySpark schema: {e}")
        raise

def generate_schema_file(schema, schema_file_path):
    """
    Generate a Python file that contains the schema definition.

    Args:
        schema (StructType): PySpark StructType schema.
        schema_file_path (str): Path to the output Python file.
    """
    try:
        logging.info("Generating schema code.")
        schema_code = f"from pyspark.sql.types import StructType\n\n"
        schema_json = json.dumps(schema.jsonValue(), indent=4).replace('true', 'True').replace('false', 'False')
        schema_code += f"def get_schema():\n"
        schema_code += f"    return StructType.fromJson({schema_json})\n"
        
        logging.info(f"Writing schema code to file: {schema_file_path}")
        with open(schema_file_path, 'w') as file:
            file.write(schema_code)
        logging.info("Schema code written successfully.")
    except Exception as e:
        logging.error(f"Error generating schema file at '{schema_file_path}': {e}")
        raise

def extract_column_paths(schema, parent_path=""):
    """
    Extract column paths and explode paths from the schema.

    Args:
        schema (StructType): PySpark StructType schema.
        parent_path (str): Path prefix for nested fields.

    Returns:
        tuple: A tuple containing:
            - List[str]: Column paths.
            - List[tuple]: Explode paths (original_path, exploded_path).
    """
    try:
        column_paths = []
        explode_paths = []
        for field in schema.fields:
            current_path = f"{parent_path}.{field.name}" if parent_path else field.name
            if isinstance(field.dataType, StructType):
                nested_paths, nested_explode = extract_column_paths(field.dataType, current_path)
                column_paths.extend(nested_paths)
                explode_paths.extend(nested_explode)
            elif isinstance(field.dataType, ArrayType):
                if isinstance(field.dataType.elementType, StructType):
                    explode_col_name = f"{camel_to_snake(field.name)}_exploded"
                    explode_paths.append((current_path, explode_col_name))
                    nested_paths, nested_explode = extract_column_paths(field.dataType.elementType, f"{explode_col_name}")
                    column_paths.extend(nested_paths)
                    explode_paths.extend(nested_explode)
                elif isinstance(field.dataType.elementType, StringType):
                    column_paths.append(current_path)
                else:
                    # Handle other types if needed
                    pass
            else:
                column_paths.append(current_path)
        return column_paths, explode_paths
    except Exception as e:
        logging.error(f"Error extracting column paths: {e}")
        raise

def update_column_paths(column_paths, explode_paths):
    """
    Update column paths with exploded column names.

    Args:
        column_paths (List[str]): A list of original column paths.
        explode_paths (List[Tuple[str, str]]): A list of tuples (original_path, exploded_path).

    Returns:
        List[str]: Updated column paths after explosions.
    """
    exploded_col_map = {orig_col: f"exploded_{exp_col_name}" for orig_col, exp_col_name in explode_paths}
    updated_column_paths = []
    for path in column_paths:
        parts = path.split('.')
        updated_parts = [exploded_col_map.get(part, part) for part in parts]
        updated_column_paths.append('.'.join(updated_parts))
    return updated_column_paths

def generate_select_expression_from_list(column_names, explode_paths, start_path):
    """
    Generate a DataFrame select expression with properly aliased columns from a list of column names.

    Args:
        column_names (List[str]): List of column paths.
        explode_paths (List[tuple]): List of explode paths (original_path, exploded_path).
        start_path (str): The starting path to prepend to each column.

    Returns:
        tuple: Comma-separated select expressions and list of alias names.
    """
    try:
        select_expr = []
        alias_names = []
        exploded_col_names = {exp[1] for exp in explode_paths}

        for col_name in column_names:
            if isinstance(col_name, str):
                parts = col_name.split('.')
                snake_case_name = camel_to_snake(parts[-1])  # Convert it to snake_case

                # Include the parent's name in the alias
                if len(parts) > 1:
                    parent_name = camel_to_snake(parts[-2])
                    snake_case_name = f"{parent_name}_{snake_case_name}"
                if start_path:
                    parent_name = camel_to_snake(start_path.split('.')[-1])
                    snake_case_name = f"{parent_name}_{snake_case_name}"

                select_expr.append(f"col('{col_name}').alias('{snake_case_name}')")
                alias_names.append(snake_case_name)
            else:
                raise ValueError("Column name is not a string.")

        return ",\n".join(select_expr), alias_names
    except Exception as e:
        logging.error(f"Error generating select expression: {e}")
        raise

def convert_array_columns_to_string(df):
    """
    Convert array columns to string representation by concatenating elements.

    Args:
        df (DataFrame): Input DataFrame.

    Returns:
        DataFrame: DataFrame with array columns converted to string.
    """
    try:
        for col_name in df.columns:
            if isinstance(df.schema[col_name].dataType, ArrayType) and isinstance(df.schema[col_name].dataType.elementType, StringType):
                df = df.withColumn(col_name, concat_ws(",", col(col_name)))
        return df
    except Exception as e:
        logging.error(f"Error converting array columns to string: {e}")
        raise

def generate_yaml_with_column_lists(column_paths, explode_paths, yaml_file_path):
    """
    Generate a YAML file containing lists of column names for explosion and selection.

    Args:
        column_paths (List[str]): List of column paths.
        explode_paths (List[tuple]): List of explode paths (original_path, exploded_path).
        yaml_file_path (str): Path to the output YAML file.
    """
    try:
        explode_columns = [exp[1] for exp in explode_paths]
        select_columns = column_paths

        yaml_content = {
            'explode_columns': explode_columns,
            'select_columns': select_columns
        }

        with open(yaml_file_path, 'w') as file:
            yaml.dump(yaml_content, file)
        
        logging.info(f"YAML file with column lists generated at: {yaml_file_path}")
    except Exception as e:
        logging.error(f"Error generating YAML file with column lists: {e}")
        raise

def generate_pyspark_script(config, schema):
    """
    Generate a PySpark script based on the given configuration and schema.

    Args:
        config (dict): Configuration dictionary.
        schema (StructType): PySpark StructType schema.
    """
    try:
        schema_file_path = config['pyspark_schema_path']
        output_file_path = config['pyspark_script_output_path']
        column_lists_yaml_path = config['column_lists_yaml_path']
        schema_module = Path(schema_file_path).stem
        start_path = config.get('start_path', '')

        logging.info(f"Generating schema file at: {schema_file_path}")
        generate_schema_file(schema, schema_file_path)
        logging.info("Schema file generated successfully.")

        logging.info("Extracting column paths and explode paths from the schema.")
        column_paths, explode_paths = extract_column_paths(schema)
        logging.info(f"Column paths: {column_paths}")
        logging.info(f"Explode paths: {explode_paths}")

        # Generate the YAML file with column lists
        logging.info(f"Generating YAML file with column lists at: {column_lists_yaml_path}")
        generate_yaml_with_column_lists(column_paths, explode_paths, column_lists_yaml_path)
        logging.info("YAML file with column lists generated successfully.")

        # Generate explosion code for arrays
        logging.info("Generating explosion code for arrays.")

        explosion_code = []
        exploded_col_map = {}

        for orig_col, exp_col_name in explode_paths:
            # Add explosion code
            explosion_code.append(f"df = df.withColumn('{exp_col_name}', explode_outer(col('{orig_col}')))")
            explosion_code.append(f"df = debug_df(df, 'After exploding {exp_col_name}')")  # Debug info if needed

        updated_column_paths = update_column_paths(column_paths, explode_paths)

        print(f"This is updated_column_paths : {updated_column_paths}")

        # Generate the explosion code string for insertion into the script
        explosion_code_str = "\n".join(explosion_code)
        logging.info(f"Explosion code: {explosion_code_str}")

        # Generate select statement
        select_expr_str, alias_names = generate_select_expression_from_list(updated_column_paths, explode_paths, start_path)

        pyspark_code = f"""
import json
import re
import yaml
from pathlib import Path
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, explode_outer, concat_ws
from pyspark.sql.types import *
import logging

logging.basicConfig(level=logging.INFO)

def debug_df(df, step_desc):
    print(f"\\n--- {{step_desc}} ---")
    df.printSchema()
    df.show(truncate=False)
    return df

def convert_array_columns_to_string(df):
    for col_name in df.columns:
        if isinstance(df.schema[col_name].dataType, ArrayType) and isinstance(df.schema[col_name].dataType.elementType, StringType):
            df = df.withColumn(col_name, concat_ws(",", col(col_name)))
    return df

spark = SparkSession.builder.appName("JsonToDataFrame").getOrCreate()

# Disable native Hadoop IO on Windows to avoid native IO issues
spark.conf.set("spark.hadoop.io.nativeio.disable", "true")
spark.conf.set("spark.hadoop.io.nativeio.use", "false")

from {schema_module} import get_schema
schema = get_schema()
df = spark.read.schema(schema).option("multiline", "true").json("{config['json_data_input_path']}")

{explosion_code_str}

df = convert_array_columns_to_string(df)

# Select and alias columns as required
select_statement = df.select(
    {select_expr_str}
)

# Show the results
select_statement.show(truncate=False)

# Write DataFrame to CSV
try:
    select_statement.write.csv("{config['output_csv_path']}", header=True, mode="overwrite")
    logging.info(f"DataFrame successfully written to CSV at: {config['output_csv_path']}")
except Exception as e:
    raise

# List of alias names
alias_names = {alias_names}
print("Alias Names:", alias_names)
"""

        logging.info(f"Generating PySpark script file at: {output_file_path}")
        with open(output_file_path, 'w') as file:
            file.write(pyspark_code)
        logging.info("PySpark script file generated successfully.")
    except Exception as e:
        logging.error(f"Error generating PySpark script: {e}")
        raise

def navigate_to_path(schema, path):
    """
    Navigate to the desired path in the schema.

    Args:
        schema (StructType): PySpark StructType schema.
        path (str): Dot-separated path to navigate to.

    Returns:
        StructType: Subschema at the given path.
    """
    if not path:
        return schema

    parts = path.split('.')
    for part in parts:
        field = next((f for f in schema.fields if f.name == part), None)
        if field and isinstance(field.dataType, StructType):
            schema = field.dataType
        else:
            raise KeyError(f"No StructField named {part}")
    return schema

if __name__ == "__main__":
    try:
        config_path = 'config.yaml'
        
        logging.info(f"Reading configuration from: {config_path}")
        with open(config_path, 'r') as file:
            config = yaml.safe_load(file)
        logging.info(f"Configuration read successfully: {config}")

        logging.info(f"Reading JSON schema from: {config['json_schema_path']}")
        json_schema = read_json_file(config['json_schema_path'])
        logging.info("JSON schema read successfully.")

        logging.info("Generating PySpark schema.")
        pyspark_schema = generate_pyspark_schema(json_schema)
        logging.info("PySpark schema generated successfully.")

        # Navigate to the desired path in the schema
        start_path = config.get('start_path', '')
        if start_path:
            logging.info(f"Navigating to start path: {start_path}")
            pyspark_schema = navigate_to_path(pyspark_schema, start_path)
            logging.info(f"Schema at start path '{start_path}' navigated successfully.")

        logging.info("Generating PySpark script.")
        generate_pyspark_script(config, pyspark_schema)
        logging.info("PySpark script generation completed.")
    except Exception as e:
        logging.error(f"Error in main execution: {e}")
        raise
